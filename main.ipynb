{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FQXM42BLnCz"
   },
   "source": [
    "# **Learn OpenCV for Computer Vision**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRMkSSIrVt9W"
   },
   "source": [
    "<img src=\"media/computer-vision.gif\" height=\"200\">\n",
    "<img src=\"media/tennis.gif\" height=\"200\">\n",
    "\n",
    "### **What is OpenCV?**\n",
    "<img src=\"media/opencv-logo.png\" height=\"200\">\n",
    "\n",
    "OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n",
    "\n",
    "\n",
    "> The library has **more than 2500** optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms.\n",
    "\n",
    " These algorithms can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc. OpenCV has more than 47 thousand people of user community and estimated number of downloads exceeding 18 million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "879mmxgoNUFA"
   },
   "source": [
    "## **Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu_W4CA6OQ1u"
   },
   "source": [
    "1. Image and Video in Computer Vision\n",
    "2. Basic Function OpenCV\n",
    "3. Edge Detection\n",
    "4. Drawing Shapes and Text\n",
    "5. Warp Perspective\n",
    "6. Joining Images\n",
    "7. Camera Settings\n",
    "8. Color Detection\n",
    "9. Contour\n",
    "10. Project Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSfESgU9T6GX"
   },
   "source": [
    "## **Image and Video in Computer Vision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/image-representation.jpg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKee_Gr1UH3m"
   },
   "source": [
    "### **What is Image?**\n",
    "Computers typically represent images as a grid of pixels, with each pixel containing information about its color and intensity. Images can be grayscale (black and white) or in color, where each pixelâ€™s color is represented by three values for red, green, and blue (RGB). An image is a 2D array of numbers stored in a raster. Each cell in the raster is called a pixel. Every image that we see in this world is nothing, but an array of numbers with various value ranges depending on the colorspaces we are using.\n",
    "### **What is Video?**\n",
    "A video is essentially a sequence of images displayed rapidly in succession to create the illusion of motion. Each individual image in this sequence is called a frame. By displaying these frames at a certain frame rate (typically measured in frames per second, or fps), videos create a smooth, continuous visual experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdP_A6ZMp2Dy"
   },
   "source": [
    "## **Basic Function OpenCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMUWq9BCqDt2"
   },
   "source": [
    "### Import OpenCV Libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1Yn6Tv7qqLWG"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StfE42lNqV6Q"
   },
   "source": [
    "### Read Input from Image, Video, and WebCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "id": "cVfBDIypqfD_",
    "outputId": "66d8e74e-d65e-4039-b1a4-8b92ceac2289"
   },
   "outputs": [],
   "source": [
    "# Read from Image\n",
    "img = cv2.imread(\"resource/cat.jpg\")\n",
    "\n",
    "cv2.imshow(\"Wilson the ichiro Cat\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Webcam\n",
    "frameWidth = 640\n",
    "frameHeight = 480\n",
    "cap = cv2.VideoCapture(0) # change 0 to your webcam index\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight)\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    cv2.imshow(\"WebCam\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Setting Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0. CV_CAP_PROP_POS_MSEC        : Current position of the video file in milliseconds.\n",
    "1. CV_CAP_PROP_POS_FRAMES      : 0-based index of the frame to be decoded/captured next.\n",
    "2. CV_CAP_PROP_POS_AVI_RATIO   : Relative position of the video file\n",
    "3. CV_CAP_PROP_FRAME_WIDTH     : Width of the frames in the video stream.\n",
    "4. CV_CAP_PROP_FRAME_HEIGHT    : Height of the frames in the video stream.\n",
    "5. CV_CAP_PROP_FPS             : Frame rate.\n",
    "6. CV_CAP_PROP_FOURCC          : 4-character code of codec.\n",
    "7. CV_CAP_PROP_FRAME_COUNT     : Number of frames in the video file.\n",
    "8. CV_CAP_PROP_FORMAT          : Format of the Mat objects returned by retrieve() .\n",
    "9. CV_CAP_PROP_MODE            : Backend-specific value indicating the current capture mode.\n",
    "10. CV_CAP_PROP_BRIGHTNESS     : Brightness of the image (only for cameras).\n",
    "11. CV_CAP_PROP_CONTRAST       : Contrast of the image (only for cameras).\n",
    "12. CV_CAP_PROP_SATURATION     : Saturation of the image (only for cameras).\n",
    "13. CV_CAP_PROP_HUE            : Hue of the image (only for cameras).\n",
    "14. CV_CAP_PROP_GAIN           : Gain of the image (only for cameras).\n",
    "15. CV_CAP_PROP_EXPOSURE       : Exposure (only for cameras).\n",
    "16. CV_CAP_PROP_CONVERT_RGB    : Boolean flags indicating whether images should be converted to RGB.\n",
    "17. CV_CAP_PROP_WHITE_BALANCE  : Currently unsupported\n",
    "18. CV_CAP_PROP_RECTIFICATION  : Rectification flag for stereo cameras (note: only supported by DC1394 v 2.x backend currently)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"resource/cat.jpg\")\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "\n",
    "# Resize Image\n",
    "resizedImage = cv2.resize(img, (640, 480))\n",
    "cv2.imshow(\"Resized Image\", resizedImage)\n",
    "\n",
    "# Rotate Image\n",
    "rotatedImage = cv2.rotate(resizedImage, cv2.ROTATE_90_CLOCKWISE)\n",
    "cv2.imshow(\"Rotated Image\", rotatedImage)\n",
    "\n",
    "# Flip Image\n",
    "flippedImage = cv2.flip(resizedImage, 1)\n",
    "cv2.imshow(\"Flipped Image\", resizedImage)\n",
    "\n",
    "# Crop Image\n",
    "croppedImage = resizedImage[50:100, 200:400]\n",
    "cv2.imshow(\"Cropped Image\", croppedImage)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color and Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"resource/cat.jpg\")\n",
    "img = cv2.resize(img, (640,480))\n",
    "\n",
    "imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "imgBlur = cv2.GaussianBlur(img, (9,9), 0)\n",
    "imgMedian = cv2.medianBlur(img, 9)\n",
    "\n",
    "cv2.imshow(\"Gray Image\", imgGray)\n",
    "cv2.imshow(\"Blur Image\", imgBlur)\n",
    "cv2.imshow(\"Median Image\", imgMedian)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Image and Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved image\n"
     ]
    }
   ],
   "source": [
    "# Save Image\n",
    "img = cv2.imread(\"resource/cat.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite(\"saved_image.jpg\", img)\n",
    "\n",
    "print(\"Successfully saved image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Video\n",
    "frameWidth = 640\n",
    "frameHeight = 480\n",
    "cap = cv2.VideoCapture(0) # change 0 to your webcam index\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(\"output.mp4\", fourcc, 20.0, (frameWidth, frameHeight))\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    out.write(img)\n",
    "    cv2.imshow(\"WebCam\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Saved Video\n",
    "cap = cv2.VideoCapture(\"output.mp4\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "delay = int(1000 / fps)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    cv2.imshow(\"Result\", img)\n",
    "    if cv2.waitKey(delay) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Edge Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"resource/ganci-1.jpg\")\n",
    "\n",
    "img = cv2.resize(img, (480, 640))\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "imgCanny = cv2.Canny(img, 150, 200)\n",
    "imgDialation = cv2.dilate(imgCanny, kernel, iterations=10)\n",
    "imgEroded = cv2.erode(imgDialation, kernel, iterations=10)\n",
    "\n",
    "cv2.imshow(\"Canny Image\", imgCanny)\n",
    "cv2.imshow(\"Dialation Image\", imgDialation)\n",
    "cv2.imshow(\"Eroded Image\", imgEroded)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Drawing Shapes and Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plainImage = np.zeros((512, 512, 3), np.uint8)\n",
    "blue = (255, 0, 0)\n",
    "green = (0, 255, 0)\n",
    "red = (0, 0, 255)\n",
    "white = (255, 255, 255)\n",
    "\n",
    "cv2.line(plainImage, (50, 0), (50, 512), red, 3)\n",
    "cv2.rectangle(plainImage, (100, 100), (250, 350), green, 3)\n",
    "cv2.circle(plainImage, (400, 400), 30, blue, cv2.FILLED)\n",
    "cv2.putText(plainImage, \"ICHIRO ITS\", (300, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, white, 3)\n",
    "\n",
    "cv2.imshow(\"Image\", plainImage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Warp Perspective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def print_coordinates(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        print(f\"[{x}, {y}],\")\n",
    "        cv2.circle(img, (x, y), 5, (255, 0, 0), -1)\n",
    "        cv2.imshow(\"Image\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"resource/ganci-2.jpg\")\n",
    "cv2.imshow(\"Image\", img)\n",
    "\n",
    "cv2.setMouseCallback(\"Image\", print_coordinates)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"resource/ganci-2.jpg\")\n",
    "\n",
    "width, height = 250, 650\n",
    "coordinates = [\n",
    "  [98, 198],\n",
    "  [183, 168],\n",
    "  [234, 452],\n",
    "  [341, 391],\n",
    "]\n",
    "\n",
    "pts1 = np.float32(coordinates)\n",
    "pts2 = np.float32([[0, 0], [width, 0], [0, height], [width, height]])\n",
    "matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "imgOutput = cv2.warpPerspective(img, matrix, (width, height))\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imshow(\"Output\", imgOutput)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Joining Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def joinImages(scale, imgArray):\n",
    "    for i in range(0, len(imgArray)):\n",
    "        if imgArray[i].shape[:2] == imgArray[0].shape[:2]:\n",
    "            imgArray[i] = cv2.resize(imgArray[i], (0, 0), None, scale, scale)\n",
    "        else:\n",
    "            imgArray[i] = cv2.resize(imgArray[i], (imgArray[0].shape[1], imgArray[0].shape[0]), None, scale, scale)\n",
    "        if len(imgArray[i].shape) == 2: imgArray[i] = cv2.cvtColor(imgArray[i], cv2.COLOR_GRAY2BGR)\n",
    "    res = np.hstack(imgArray)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"resource/rubik.jpg\")\n",
    "imgFliped0 = cv2.flip(img, 0)\n",
    "imgFliped1 = cv2.flip(img, 1)\n",
    "imgFliped2 = cv2.flip(img, -1)\n",
    "\n",
    "# Using Numpy\n",
    "# stackedImage = np.hstack((imgFliped0, imgFliped1))\n",
    "# cv2.imshow(\"Stacked Image\", stackedImage)\n",
    "\n",
    "imgStack = joinImages(0.5, ([img, imgFliped0, imgFliped1, imgFliped2]))\n",
    "cv2.imshow(\"ImageStack\", imgStack)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Camera Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 50, 64]\r"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:2561: error: (-27:Null pointer) NULL window: 'Camera Settings' in function 'cvGetTrackbarPos'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m brightness \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetTrackbarPos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBrightness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCamera Settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m contrast \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetTrackbarPos(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContrast\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamera Settings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m saturation \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetTrackbarPos(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaturation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamera Settings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:2561: error: (-27:Null pointer) NULL window: 'Camera Settings' in function 'cvGetTrackbarPos'\n"
     ]
    }
   ],
   "source": [
    "def empty(a):\n",
    "    pass\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 480)\n",
    "\n",
    "brightness = cap.get(cv2.CAP_PROP_BRIGHTNESS)\n",
    "contrast = cap.get(cv2.CAP_PROP_CONTRAST)\n",
    "saturation = cap.get(cv2.CAP_PROP_SATURATION)\n",
    "\n",
    "cv2.namedWindow(\"Camera Settings\")\n",
    "cv2.resizeWindow(\"Camera Settings\", 640, 120)\n",
    "cv2.createTrackbar(\"Brightness\", \"Camera Settings\", int(brightness), 255, empty)\n",
    "cv2.createTrackbar(\"Contrast\", \"Camera Settings\", int(contrast), 100, empty)\n",
    "cv2.createTrackbar(\"Saturation\", \"Camera Settings\", int(saturation), 100, empty)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    brightness = cv2.getTrackbarPos(\"Brightness\", \"Camera Settings\")\n",
    "    contrast = cv2.getTrackbarPos(\"Contrast\", \"Camera Settings\")\n",
    "    saturation = cv2.getTrackbarPos(\"Saturation\", \"Camera Settings\")\n",
    "\n",
    "    print(f\"\\r[{brightness}, {contrast}, {saturation}]\", end=\"\\r\")\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BRIGHTNESS, brightness)\n",
    "    cap.set(cv2.CAP_PROP_CONTRAST, contrast)\n",
    "    cap.set(cv2.CAP_PROP_SATURATION, saturation)\n",
    "\n",
    "    cv2.imshow(\"WebCam\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Color Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/hsv-color-space.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 44, 49, 122, 217, 207]     \r"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:2561: error: (-27:Null pointer) NULL window: 'Color Settings' in function 'cvGetTrackbarPos'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# success, img = cap.read()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# img = cv2.flip(img, 1)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m imgHSV \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2HSV)\n\u001b[1;32m---> 27\u001b[0m h_min \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetTrackbarPos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHue Min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColor Settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m h_max \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetTrackbarPos(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHue Max\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColor Settings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m s_min \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetTrackbarPos(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSat Min\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColor Settings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:2561: error: (-27:Null pointer) NULL window: 'Color Settings' in function 'cvGetTrackbarPos'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def empty(a):\n",
    "    pass\n",
    "\n",
    "cv2.namedWindow(\"Color Settings\")\n",
    "cv2.resizeWindow(\"Color Settings\", 640, 240)\n",
    "cv2.createTrackbar(\"Hue Min\", \"Color Settings\", 0, 179, empty)\n",
    "cv2.createTrackbar(\"Hue Max\", \"Color Settings\", 179, 179, empty)\n",
    "cv2.createTrackbar(\"Sat Min\", \"Color Settings\", 0, 255, empty)\n",
    "cv2.createTrackbar(\"Sat Max\", \"Color Settings\", 255, 255, empty)\n",
    "cv2.createTrackbar(\"Val Min\", \"Color Settings\", 0, 255, empty)\n",
    "cv2.createTrackbar(\"Val Max\", \"Color Settings\", 255, 255, empty)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_BRIGHTNESS, 93)\n",
    "cap.set(cv2.CAP_PROP_CONTRAST, 32)\n",
    "cap.set(cv2.CAP_PROP_SATURATION, 52)\n",
    "\n",
    "while True:\n",
    "    img = cv2.imread(\"resource/rubik.jpg\")\n",
    "\n",
    "    # success, img = cap.read()\n",
    "    # img = cv2.flip(img, 1)\n",
    "\n",
    "    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h_min = cv2.getTrackbarPos(\"Hue Min\", \"Color Settings\")\n",
    "    h_max = cv2.getTrackbarPos(\"Hue Max\", \"Color Settings\")\n",
    "    s_min = cv2.getTrackbarPos(\"Sat Min\", \"Color Settings\")\n",
    "    s_max = cv2.getTrackbarPos(\"Sat Max\", \"Color Settings\")\n",
    "    v_min = cv2.getTrackbarPos(\"Val Min\", \"Color Settings\")\n",
    "    v_max = cv2.getTrackbarPos(\"Val Max\", \"Color Settings\")\n",
    "\n",
    "    print(f\"\\r[{h_min}, {s_min}, {v_min}, {h_max}, {s_max}, {v_max}]   \", end=\"\\r\")\n",
    "\n",
    "    lower = np.array([h_min, s_min, v_min])\n",
    "    upper = np.array([h_max, s_max, v_max])\n",
    "    mask = cv2.inRange(imgHSV, lower, upper)\n",
    "    imgResult = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "    imgStack = joinImages(0.6, [img, imgHSV, mask, imgResult])\n",
    "    cv2.imshow(\"Images\", imgStack)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 80, y: 232, w: 138, h: 232\r"
     ]
    }
   ],
   "source": [
    "def getContours(img):\n",
    "    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    x, y, w, h = 0, 0, 0, 0\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 250:\n",
    "            cv2.drawContours(imgResult, cnt, -1, (255, 0, 0), 3)\n",
    "            peri = cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, 0.02*peri, True)\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "            print(f\"\\rx: {x}, y: {y}, w: {w}, h: {h}\", end=\"\\r\")\n",
    "\n",
    "img = cv2.imread(\"resource/rubik.jpg\")\n",
    "color = np.array([92, 170, 0, 116, 255, 141])\n",
    "lower = np.array(color[0:3])\n",
    "upper = np.array(color[3:6])\n",
    "mask = cv2.inRange(cv2.cvtColor(img, cv2.COLOR_BGR2HSV), lower, upper)\n",
    "imgResult = img.copy()\n",
    "getContours(mask)\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imshow(\"Result\", imgResult)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Code Together\n",
    "import cv2\n",
    "import numpy as np\n",
    "frameWidth = 640\n",
    "frameHeight = 480\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight)\n",
    "cap.set(cv2.CAP_PROP_FPS, 15)\n",
    "\n",
    "myColors = [[96, 66, 61, 119, 235, 253]] \n",
    "myColorValues = [[0,0,200]]          ## BGR\n",
    "\n",
    "myPoints =  []  ## [x , y , colorId ]\n",
    "\n",
    "def findColor(img,myColors,myColorValues):\n",
    "    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    count = 0\n",
    "    newPoints=[]\n",
    "    for color in myColors:\n",
    "        lower = np.array(color[0:3])\n",
    "        upper = np.array(color[3:6])\n",
    "        mask = cv2.inRange(imgHSV,lower,upper)\n",
    "        x,y=getContours(mask)\n",
    "        cv2.circle(imgResult,(x,y),15,myColorValues[count],cv2.FILLED)\n",
    "        if x!=0 and y!=0:\n",
    "            newPoints.append([x,y,count])\n",
    "        count +=1\n",
    "        cv2.imshow(str(color[0]),mask)\n",
    "    return newPoints\n",
    "\n",
    "def getContours(img):\n",
    "    contours,hierarchy = cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    x,y,w,h = 0,0,0,0\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area>250:\n",
    "            cv2.drawContours(imgResult, cnt, -1, (255, 0, 0), 3)\n",
    "            peri = cv2.arcLength(cnt,True)\n",
    "            approx = cv2.approxPolyDP(cnt,0.02*peri,True)\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "    return x+w//2,y\n",
    "\n",
    "def drawOnCanvas(myPoints, myColorValues):\n",
    "    for i in range(len(myPoints)):\n",
    "        point = myPoints[i]\n",
    "        cv2.circle(imgResult, (point[0], point[1]), 8, myColorValues[point[2]], cv2.FILLED)\n",
    "\n",
    "        # if i > 0:\n",
    "        #     prev_point = myPoints[i - 1]\n",
    "        #     cv2.line(imgResult, (prev_point[0], prev_point[1]), (point[0], point[1]), myColorValues[point[2]], 5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    imgResult = img.copy()\n",
    "    newPoints = findColor(img, myColors, myColorValues)\n",
    "    if len(newPoints)!=0:\n",
    "        for newP in newPoints:\n",
    "            myPoints.append(newP)\n",
    "    if len(myPoints)!=0:\n",
    "        drawOnCanvas(myPoints,myColorValues)\n",
    "\n",
    "    imgResult = cv2.flip(imgResult, 1)\n",
    "\n",
    "    cv2.imshow(\"Result\", imgResult)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "        myPoints = []\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ai itu belajar secara mandiri \n",
    "ai itu bisa bahasa alami nlp mengenali gambar dan video\n",
    "ai itu bisa menegenali sesuatu \n",
    "# macam macam bidang ai\n",
    "<!-- machine learning -->\n",
    "belejar seeprti manusia\n",
    "dataset -> ml -> knowladge\n",
    "1. dataset\n",
    "dataset ada 2 => tersturktur dan tidak tersetruktur\n",
    "dimensi data ekstrak atribut \n",
    "data terstruk wi kayak dataframe\n",
    "data tidak terstruktur wi kayak gambar, video dll.\n",
    "2. ml\n",
    "ml algorithm ada 5 => klasifikasi, klustering, asosiasi, estimasi, forecasting.\n",
    "contoh = supervised, unsupervised, reinforcment\n",
    "ds wi ada 2 gaya => ai(mencoba dulu) dan statistik(langsung dipikir dulu baru dijalankan )\n",
    "3. knowladge\n",
    "skip\n",
    "4. ml aplication\n",
    "a. struktur\n",
    "menemukan pola sifat seseorang dari data pribadinya seperti harta, tindak kriminal, penegak hukum, suka nonton apa, denger apa dll.\n",
    "nanti datanya bisa dipake netflix/ amazon untuk memberikan rekomendasi atau lain sebahainya.\n",
    "b. unstruktur\n",
    "skip\n",
    "<!-- nlp -->\n",
    "dataset -> text processing -> ml algo -> knowladge\n",
    "1. text processing\n",
    "tokenization ->streanubg - >stopword finishing.\n",
    "2. ml algo \n",
    "transformer(algoritmanya chatgpt), altention.\n",
    "dataset chatgpt => wiki, reddit, common crawl, news, web text, books.\n",
    "3. prompt\n",
    "cstome intructions\n",
    "\n",
    "\n",
    "\n",
    "<!-- cv -->\n",
    "dataset -> image/video processing -> ml algo -> knowladge\n",
    "<!-- planing and optimittaion -->\n",
    "algoritma genetic, que\n",
    "buat bisnis proses jadi kita bisa tau berapa tenaga kerja waktu yang harus dikeluarkan dalam sebuah bisnis\n",
    "buat menganalisis \n",
    "membangun struktur organisasi\n",
    "<!-- expert systematic -->\n",
    "sistem yang ceradas kayak netflix bisa memberikan rekomendasi pada user / pengguna\n",
    "<!-- robotic -->\n",
    "skip\n",
    "<!-- abis -->\n",
    "\n",
    "# informasi\n",
    "data -> informasi -> pengetahuan -> kebijakan\n",
    "data itu cuman kek sehari harinya\n",
    "kalo informasi itu udah diolah datanya jadi kesimpulan\n",
    "kalo pengetahuan itu sebuah pola kebiasaan dari informasi yang di dapat\n",
    "kebijakan itu sebuah wisedom yang diberikan untuk hal umum melalui pengetahuan yang telah diberikan.\n",
    "\n",
    "# use case data sains\n",
    "1. regulation improvemnet\n",
    "2. softwere imporvement\n",
    "\n",
    "# 8 tahap penelitian\n",
    "1. pahami prinsip dan konsep penelitian\n",
    "2\n",
    "\n",
    "<!-- kata ga paham -->\n",
    "fine tuning\n",
    "llm\n",
    "<!-- kata kata -->\n",
    "\"manusia itu cerdas tapi lambat, komputer itu cepat tapi bodoh\"\n",
    "\"semakin ngerti sesuatu semakin kita tidak yakin\"\n",
    "\"coding is dead\"\n",
    "\"prompt itu memiliki seni\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
